# Copyright 2025 AGI System Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# è¾“å‡ºå¤„ç†å™¨ | Output Processor
import base64
import json
from typing import Dict, Any
from .language_resources import get_string

class OutputProcessor:
    def __init__(self, language='zh'):
        self.language = language
        # è¾“å‡ºé£æ ¼æ˜ å°„ | Output style mapping
        self.output_styles = {
            "neutral": {"tone": "professional", "complexity": "medium"},
            "happy": {"tone": "friendly", "complexity": "simple"},
            "sad": {"tone": "compassionate", "complexity": "simple"},
            "angry": {"tone": "direct", "complexity": "simple"},
            "excited": {"tone": "enthusiastic", "complexity": "simple"},
            "calm": {"tone": "reassuring", "complexity": "medium"}
        }
        # æƒ…æ„Ÿè¡¨æƒ…æ˜ å°„ | Emotion emoji mapping
        self.emotion_emojis = {
            "neutral": "ğŸ˜",
            "happy": "ğŸ˜Š",
            "sad": "ğŸ˜¢",
            "angry": "ğŸ˜ ",
            "excited": "ğŸ¤©",
            "calm": "ğŸ˜Œ"
        }
    
    def set_language(self, lang: str):
        """è®¾ç½®å¤„ç†å™¨è¯­è¨€ | Set processor language"""
        if lang not in ['zh', 'en']:
            raise ValueError(get_string("unsupported_language", self.language).format(lang=lang))
        self.language = lang
    
    def process_output(self, output_data: Dict, system_emotion: str, output_emotion: str) -> Dict:
        """å¤„ç†æ¨¡å‹è¾“å‡ºå¹¶ç”Ÿæˆæœ€ç»ˆå“åº” | Process model output and generate final response
        å‚æ•°:
            output_data: æ¨¡å‹è¾“å‡ºæ•°æ® | Model output data
            system_emotion: ç³»ç»Ÿå½“å‰æƒ…æ„ŸçŠ¶æ€ | Current system emotion state
            output_emotion: è¾“å‡ºå†…å®¹çš„æƒ…æ„Ÿ | Emotion in output content
        è¿”å›:
            æœ€ç»ˆç”¨æˆ·å“åº”å­—å…¸ | Final user response dictionary
        """
        # è·å–è¾“å‡ºé£æ ¼ | Get output style
        style = self.output_styles.get(system_emotion, self.output_styles["neutral"])
        
        # ç”ŸæˆåŸºç¡€æ–‡æœ¬å“åº” | Generate base text response
        text_response = self._generate_text_response(output_data, style)
        
        # æ·»åŠ æƒ…æ„Ÿè¡¨æƒ… | Add emotion emoji
        emoji = self.emotion_emojis.get(output_emotion, "")
        if emoji:
            text_response = f"{emoji} {text_response}"
        
        # æ„å»ºå“åº”å¯¹è±¡ | Build response object
        response = {
            "text": text_response,
            "emotion": output_emotion,
            "system_emotion": system_emotion,
            "language": self.language,
            "timestamp": time.time()
        }
        
        # æ·»åŠ å¤šåª’ä½“è¾“å‡ºï¼ˆå¦‚æœè¯·æ±‚ï¼‰| Add multimedia output if requested
        if "output_format" in output_data:
            if "audio" in output_data["output_format"]:
                response["audio"] = self._generate_audio_output(text_response, output_emotion)
            if "image" in output_data["output_format"]:
                response["image"] = self._generate_image_output(text_response, output_emotion)
        
        return response
    
    def _generate_text_response(self, output_data: Dict, style: Dict) -> str:
        """ç”Ÿæˆæ–‡æœ¬å“åº” | Generate text response
        å‚æ•°:
            output_data: æ¨¡å‹è¾“å‡ºæ•°æ® | Model output data
            style: è¾“å‡ºé£æ ¼ | Output style
        è¿”å›:
            æ ¼å¼åŒ–æ–‡æœ¬ | Formatted text
        """
        # è·å–åŸºç¡€å†…å®¹ | Get base content
        content = output_data.get("content", "")
        
        # æ ¹æ®é£æ ¼è°ƒæ•´å†…å®¹ | Adjust content based on style
        if style["tone"] == "friendly":
            prefix = get_string("friendly_prefix", self.language)
            content = f"{prefix} {content}"
        elif style["tone"] == "compassionate":
            prefix = get_string("compassionate_prefix", self.language)
            content = f"{prefix} {content}"
        elif style["tone"] == "direct":
            content = f"â— {content}"
        
        # ç®€åŒ–å¤æ‚å†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰| Simplify complex content if needed
        if style["complexity"] == "simple" and len(content.split()) > 20:
            # åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨æ‘˜è¦æ¨¡å‹ | In real system, use summarization model
            content = content[:150] + "..." if len(content) > 150 else content
        
        return content
    
    def _generate_audio_output(self, text: str, emotion: str) -> Dict:
        """ç”ŸæˆéŸ³é¢‘è¾“å‡º | Generate audio output
        å‚æ•°:
            text: è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬ | Text to convert to speech
            emotion: æƒ…æ„ŸçŠ¶æ€ | Emotion state
        è¿”å›:
            éŸ³é¢‘æ•°æ®å­—å…¸ | Audio data dictionary
        """
        # åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨éŸ³é¢‘å¤„ç†æ¨¡å‹ | In real system, call audio processing model
        # è¿”å›å ä½ç¬¦ | Return placeholder
        return {
            "format": "wav",
            "duration": len(text) * 0.1,  # ä¼°è®¡æŒç»­æ—¶é—´ | Estimated duration
            "emotion": emotion,
            "placeholder": True,
            "message": get_string("audio_placeholder", self.language)
        }
    
    def _generate_image_output(self, text: str, emotion: str) -> Dict:
        """ç”Ÿæˆå›¾åƒè¾“å‡º | Generate image output
        å‚æ•°:
            text: å›¾åƒæè¿°æ–‡æœ¬ | Image description text
            emotion: æƒ…æ„ŸçŠ¶æ€ | Emotion state
        è¿”å›:
            å›¾åƒæ•°æ®å­—å…¸ | Image data dictionary
        """
        # åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨å›¾åƒç”Ÿæˆæ¨¡å‹ | In real system, call image generation model
        # è¿”å›å ä½ç¬¦ | Return placeholder
        return {
            "format": "png",
            "width": 256,
            "height": 256,
            "emotion": emotion,
            "placeholder": True,
            "message": get_string("image_placeholder", self.language)
        }
    
    def generate_error_response(self, error_message: str) -> Dict:
        """ç”Ÿæˆé”™è¯¯å“åº” | Generate error response
        å‚æ•°:
            error_message: é”™è¯¯æ¶ˆæ¯ | Error message
        è¿”å›:
            é”™è¯¯å“åº”å­—å…¸ | Error response dictionary
        """
        return {
            "error": True,
            "message": error_message,
            "language": self.language,
            "suggestions": [
                get_string("error_suggestion_retry", self.language),
                get_string("error_suggestion_check_input", self.language),
                get_string("error_suggestion_contact_support", self.language)
            ],
            "timestamp": time.time()
        }
    
    def format_output(self, output_data: Dict, format_type: str = "text") -> Any:
        """æ ¼å¼åŒ–è¾“å‡ºä¸ºæŒ‡å®šç±»å‹ | Format output to specified type
        å‚æ•°:
            output_data: è¾“å‡ºæ•°æ® | Output data
            format_type: è¾“å‡ºæ ¼å¼ (text/json/html) | Output format
        è¿”å›:
            æ ¼å¼åŒ–åçš„è¾“å‡º | Formatted output
        """
        if format_type == "json":
            return json.dumps(output_data, ensure_ascii=False, indent=2)
        
        if format_type == "html":
            return self._format_html_output(output_data)
        
        # é»˜è®¤è¿”å›æ–‡æœ¬ | Default to text
        return output_data.get("text", get_string("no_text_output", self.language))
    
    def _format_html_output(self, output_data: Dict) -> str:
        """å°†è¾“å‡ºæ ¼å¼åŒ–ä¸ºHTML | Format output as HTML
        å‚æ•°:
            output_data: è¾“å‡ºæ•°æ® | Output data
        è¿”å›:
            HTMLå­—ç¬¦ä¸² | HTML string
        """
        emotion = output_data.get("emotion", "neutral")
        emoji = self.emotion_emojis.get(emotion, "")
        
        html = f"""
        <div class="system-output" data-emotion="{emotion}">
            <div class="emotion-indicator">{emoji}</div>
            <div class="output-content">
                <p>{output_data.get('text', '')}</p>
        """
        
        if "audio" in output_data:
            html += f"""
                <div class="audio-output">
                    <p>{get_string("audio_output_available", self.language)}</p>
                </div>
            """
        
        if "image" in output_data:
            html += f"""
                <div class="image-output">
                    <p>{get_string("image_output_available", self.language)}</p>
                </div>
            """
        
        html += """
            </div>
        </div>
        """
        return html
