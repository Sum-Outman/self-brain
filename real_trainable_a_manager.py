#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®å¯è®­ç»ƒçš„Aç®¡ç†æ¨¡å‹ - å®Œæ•´ç¥ç»ç½‘ç»œæ¶æ„
ä¿ç•™æ‰€æœ‰ç°æœ‰åŠŸèƒ½çš„åŒæ—¶ï¼Œå®ç°çœŸæ­£çš„å¯è®­ç»ƒAIæ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import time
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import asyncio
import websockets
from datetime import datetime
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    input_dim: int = 768
    hidden_dim: int = 512
    emotion_dim: int = 8
    num_models: int = 11
    num_layers: int = 6
    num_heads: int = 8
    dropout: float = 0.1
    learning_rate: float = 1e-4
    batch_size: int = 32
    epochs: int = 100
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'

class EmotionalStateNetwork(nn.Module):
    """æƒ…æ„ŸçŠ¶æ€ç½‘ç»œ - å¯è®­ç»ƒçš„8ç»´æƒ…æ„Ÿæ¨¡å‹"""
    
    def __init__(self, input_dim: int, emotion_dim: int):
        super().__init__()
        self.emotion_dim = emotion_dim
        
        # æƒ…æ„Ÿç‰¹å¾æå–
        self.emotion_encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, emotion_dim),
            nn.Sigmoid()  # è¾“å‡º0-1ä¹‹é—´çš„æƒ…æ„Ÿå¼ºåº¦
        )
        
        # æƒ…æ„ŸåŠ¨æ€å˜åŒ–ç½‘ç»œ
        self.emotion_dynamics = nn.GRU(
            input_size=emotion_dim + input_dim,
            hidden_size=128,
            num_layers=2,
            batch_first=True,
            dropout=0.1
        )
        
        self.emotion_predictor = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, emotion_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor, prev_emotion: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        batch_size = x.size(0)
        
        # å½“å‰æƒ…æ„ŸçŠ¶æ€
        current_emotion = self.emotion_encoder(x)
        
        # æƒ…æ„ŸåŠ¨æ€å˜åŒ–
        if prev_emotion is None:
            prev_emotion = torch.zeros(batch_size, self.emotion_dim, device=x.device)
        
        # ç»„åˆè¾“å…¥
        combined_input = torch.cat([current_emotion, x], dim=-1).unsqueeze(1)
        
        # é€šè¿‡GRUå¤„ç†æ—¶åº
        gru_out, _ = self.emotion_dynamics(combined_input)
        gru_out = gru_out.squeeze(1)
        
        # é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»æƒ…æ„Ÿ
        predicted_emotion = self.emotion_predictor(gru_out)
        
        return {
            'current_emotion': current_emotion,
            'predicted_emotion': predicted_emotion,
            'emotion_features': gru_out
        }

class AttentionBasedModelRouter(nn.Module):
    """åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹è·¯ç”±ç½‘ç»œ"""
    
    def __init__(self, input_dim: int, num_models: int, num_heads: int = 8):
        super().__init__()
        self.num_models = num_models
        self.num_heads = num_heads
        
        # å¤šå¤´æ³¨æ„åŠ›
        self.attention = nn.MultiheadAttention(
            embed_dim=input_dim,
            num_heads=num_heads,
            dropout=0.1
        )
        
        # æ¨¡å‹é€‰æ‹©ç½‘ç»œ
        self.model_selector = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, num_models)
        )
        
        # ä»»åŠ¡é‡è¦æ€§è¯„ä¼°
        self.importance_scorer = nn.Sequential(
            nn.Linear(input_dim + num_models, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor, context: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """è·¯ç”±å†³ç­–"""
        batch_size = x.size(0)
        
        # è‡ªæ³¨æ„åŠ›å¤„ç†
        x_reshaped = x.unsqueeze(0)  # æ·»åŠ åºåˆ—ç»´åº¦
        attended, _ = self.attention(x_reshaped, x_reshaped, x_reshaped)
        attended = attended.squeeze(0)
        
        # æ¨¡å‹é€‰æ‹©æ¦‚ç‡
        model_logits = self.model_selector(attended)
        model_probs = F.softmax(model_logits, dim=-1)
        
        # ä»»åŠ¡é‡è¦æ€§
        if context is None:
            context = torch.zeros(batch_size, self.num_models, device=x.device)
        
        importance_input = torch.cat([attended, model_probs], dim=-1)
        task_importance = self.importance_scorer(importance_input)
        
        return {
            'model_probabilities': model_probs,
            'task_importance': task_importance,
            'routing_features': attended
        }

class MultimodalFusionNetwork(nn.Module):
    """å¤šæ¨¡æ€èåˆç½‘ç»œ"""
    
    def __init__(self, input_dim: int, num_modalities: int = 4):
        super().__init__()
        self.num_modalities = num_modalities
        
        # å„æ¨¡æ€ç¼–ç å™¨
        self.text_encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.audio_encoder = nn.Sequential(
            nn.Linear(128, 256),  # å‡è®¾éŸ³é¢‘ç‰¹å¾128ç»´
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.image_encoder = nn.Sequential(
            nn.Linear(512, 256),  # å‡è®¾å›¾åƒç‰¹å¾512ç»´
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.spatial_encoder = nn.Sequential(
            nn.Linear(64, 256),   # å‡è®¾ç©ºé—´ç‰¹å¾64ç»´
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # èåˆç½‘ç»œ
        self.fusion_attention = nn.MultiheadAttention(
            embed_dim=256 * num_modalities,
            num_heads=8,
            dropout=0.1
        )
        
        self.fusion_output = nn.Sequential(
            nn.Linear(256 * num_modalities, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, input_dim)
        )
    
    def forward(self, text_feat: torch.Tensor, audio_feat: Optional[torch.Tensor] = None,
                image_feat: Optional[torch.Tensor] = None, spatial_feat: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å¤šæ¨¡æ€ç‰¹å¾èåˆ"""
        
        # ç¼–ç å„æ¨¡æ€ç‰¹å¾
        text_encoded = self.text_encoder(text_feat)
        
        # å¤„ç†å¯é€‰è¾“å…¥
        batch_size = text_feat.size(0)
        device = text_feat.device
        
        if audio_feat is None:
            audio_feat = torch.zeros(batch_size, 128, device=device)
        if image_feat is None:
            image_feat = torch.zeros(batch_size, 512, device=device)
        if spatial_feat is None:
            spatial_feat = torch.zeros(batch_size, 64, device=device)
        
        audio_encoded = self.audio_encoder(audio_feat)
        image_encoded = self.image_encoder(image_feat)
        spatial_encoded = self.spatial_encoder(spatial_feat)
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        combined_features = torch.cat([
            text_encoded, audio_encoded, image_encoded, spatial_encoded
        ], dim=-1)
        
        # æ³¨æ„åŠ›èåˆ
        combined_reshaped = combined_features.unsqueeze(0)
        fused, _ = self.fusion_attention(combined_reshaped, combined_reshaped, combined_reshaped)
        fused = fused.squeeze(0)
        
        # è¾“å‡ºèåˆç‰¹å¾
        return self.fusion_output(fused)

class RealTrainableAManager(nn.Module):
    """çœŸå®å¯è®­ç»ƒçš„Aç®¡ç†æ¨¡å‹ - å®Œæ•´æ¶æ„"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # è¾“å…¥ç¼–ç å±‚
        self.input_projection = nn.Linear(config.input_dim, config.hidden_dim)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.num_heads,
            dim_feedforward=config.hidden_dim * 4,
            dropout=config.dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)
        
        # æƒ…æ„Ÿç½‘ç»œ
        self.emotion_network = EmotionalStateNetwork(config.hidden_dim, config.emotion_dim)
        
        # è·¯ç”±ç½‘ç»œ
        self.router = AttentionBasedModelRouter(config.hidden_dim, config.num_models)
        
        # å¤šæ¨¡æ€èåˆ
        self.fusion_network = MultimodalFusionNetwork(config.hidden_dim)
        
        # è¾“å‡ºå±‚
        self.output_heads = nn.ModuleDict({
            'emotions': nn.Linear(config.hidden_dim, config.emotion_dim),
            'model_weights': nn.Linear(config.hidden_dim, config.num_models),
            'task_embedding': nn.Linear(config.hidden_dim, config.input_dim),
            'confidence': nn.Linear(config.hidden_dim, 1)
        })
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1, 100, config.hidden_dim) * 0.02)
        
        self.to(config.device)
    
    def forward(self, x: torch.Tensor, task_context: Optional[Dict] = None) -> Dict[str, torch.Tensor]:
        """å®Œæ•´å‰å‘ä¼ æ’­"""
        batch_size, seq_len = x.size(0), x.size(1)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        if seq_len <= 100:
            x = x + self.pos_encoding[:, :seq_len, :]
        
        # Transformerç¼–ç 
        encoded = self.transformer(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = torch.mean(encoded, dim=1)
        
        # æƒ…æ„Ÿåˆ†æ
        emotion_results = self.emotion_network(pooled)
        
        # æ¨¡å‹è·¯ç”±
        routing_results = self.router(pooled)
        
        # å¤šæ¨¡æ€èåˆï¼ˆå¦‚æœæä¾›ä¸Šä¸‹æ–‡ï¼‰
        if task_context:
            fused = self.fusion_network(
                text_feat=pooled,
                audio_feat=task_context.get('audio'),
                image_feat=task_context.get('image'),
                spatial_feat=task_context.get('spatial')
            )
        else:
            fused = pooled
        
        # è¾“å‡ºå¤´
        outputs = {
            'emotions': torch.sigmoid(self.output_heads['emotions'](fused)),
            'model_weights': F.softmax(self.output_heads['model_weights'](fused), dim=-1),
            'task_embedding': self.output_heads['task_embedding'](fused),
            'confidence': torch.sigmoid(self.output_heads['confidence'](fused)),
            'emotion_features': emotion_results['emotion_features'],
            'routing_features': routing_results['routing_features']
        }
        
        return outputs
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.state_dict(),
            'config': self.config,
            'timestamp': datetime.now().isoformat()
        }, filepath)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.config.device)
        self.load_state_dict(checkpoint['model_state_dict'])
        logger.info(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")

class AManagerDataset(Dataset):
    """Aç®¡ç†æ¨¡å‹è®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, num_samples: int = 10000):
        self.num_samples = num_samples
        self.emotion_labels = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'trust', 'anticipation']
        self.task_types = [
            'text', 'audio', 'image', 'video', 'spatial', 'sensor', 
            'control', 'motion', 'knowledge', 'programming'
        ]
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        """ç”Ÿæˆè®­ç»ƒæ ·æœ¬"""
        # æ¨¡æ‹Ÿè¾“å…¥ç‰¹å¾
        input_features = torch.randn(50, 768)  # 50ä¸ªæ—¶é—´æ­¥ï¼Œ768ç»´ç‰¹å¾
        
        # ç”Ÿæˆæƒ…æ„Ÿæ ‡ç­¾
        emotion_labels = torch.rand(8)
        emotion_labels = emotion_labels / emotion_labels.sum()
        
        # ç”Ÿæˆæ¨¡å‹é€‰æ‹©æ ‡ç­¾
        model_labels = torch.zeros(11)
        model_idx = np.random.randint(0, 11)
        model_labels[model_idx] = 1.0
        
        # ç”Ÿæˆä»»åŠ¡é‡è¦æ€§æ ‡ç­¾
        importance_label = torch.rand(1)
        
        # ç”Ÿæˆç½®ä¿¡åº¦æ ‡ç­¾
        confidence_label = torch.rand(1)
        
        return {
            'input': input_features,
            'emotion_target': emotion_labels,
            'model_target': model_labels,
            'importance_target': importance_label,
            'confidence_target': confidence_label
        }

class AManagerTrainer:
    """Aç®¡ç†æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model: RealTrainableAManager, config: ModelConfig):
        self.model = model
        self.config = config
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=1e-4
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=10,
            T_mult=2
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion_emotion = nn.BCELoss()
        self.criterion_model = nn.CrossEntropyLoss()
        self.criterion_regression = nn.MSELoss()
        
        # æ•°æ®é›†
        self.train_dataset = AManagerDataset(num_samples=10000)
        self.val_dataset = AManagerDataset(num_samples=1000)
        
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=config.batch_size,
            shuffle=True,
            num_workers=0
        )
        
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=0
        )
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        emotion_losses = []
        model_losses = []
        importance_losses = []
        confidence_losses = []
        
        for batch_idx, batch in enumerate(self.train_loader):
            self.optimizer.zero_grad()
            
            # æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
            inputs = batch['input'].to(self.config.device)
            emotion_targets = batch['emotion_target'].to(self.config.device)
            model_targets = batch['model_target'].to(self.config.device)
            importance_targets = batch['importance_target'].to(self.config.device)
            confidence_targets = batch['confidence_target'].to(self.config.device)
            
            # æ¨¡å‹é¢„æµ‹
            outputs = self.model(inputs)
            
            # è®¡ç®—æŸå¤±
            emotion_loss = self.criterion_emotion(outputs['emotions'], emotion_targets)
            model_loss = self.criterion_model(outputs['model_weights'], model_targets.argmax(dim=1))
            importance_loss = self.criterion_regression(outputs['task_embedding'].mean(dim=1), importance_targets.squeeze())
            confidence_loss = self.criterion_regression(outputs['confidence'], confidence_targets)
            
            # æ€»æŸå¤±
            total_batch_loss = (
                emotion_loss * 0.3 +
                model_loss * 0.4 +
                importance_loss * 0.2 +
                confidence_loss * 0.1
            )
            
            total_batch_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            total_loss += total_batch_loss.item()
            emotion_losses.append(emotion_loss.item())
            model_losses.append(model_loss.item())
            importance_losses.append(importance_loss.item())
            confidence_losses.append(confidence_loss.item())
            
            num_batches += 1
            
            if batch_idx % 100 == 0:
                logger.info(
                    f"Epoch {epoch}, Batch {batch_idx}, "
                    f"Loss: {total_batch_loss.item():.4f}, "
                    f"Emotion: {emotion_loss.item():.4f}, "
                    f"Model: {model_loss.item():.4f}"
                )
        
        self.scheduler.step()
        
        return {
            'total_loss': total_loss / num_batches,
            'emotion_loss': np.mean(emotion_losses),
            'model_loss': np.mean(model_losses),
            'importance_loss': np.mean(importance_losses),
            'confidence_loss': np.mean(confidence_losses)
        }
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                inputs = batch['input'].to(self.config.device)
                emotion_targets = batch['emotion_target'].to(self.config.device)
                model_targets = batch['model_target'].to(self.config.device)
                importance_targets = batch['importance_target'].to(self.config.device)
                confidence_targets = batch['confidence_target'].to(self.config.device)
                
                outputs = self.model(inputs)
                
                emotion_loss = self.criterion_emotion(outputs['emotions'], emotion_targets)
                model_loss = self.criterion_model(outputs['model_weights'], model_targets.argmax(dim=1))
                importance_loss = self.criterion_regression(outputs['task_embedding'].mean(dim=1), importance_targets.squeeze())
                confidence_loss = self.criterion_regression(outputs['confidence'], confidence_targets)
                
                total_batch_loss = (
                    emotion_loss * 0.3 +
                    model_loss * 0.4 +
                    importance_loss * 0.2 +
                    confidence_loss * 0.1
                )
                
                total_loss += total_batch_loss.item()
                num_batches += 1
        
        return {
            'val_loss': total_loss / num_batches
        }
    
    def train(self, num_epochs: int = None):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        if num_epochs is None:
            num_epochs = self.config.epochs
        
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒçœŸå®å¯è®­ç»ƒçš„Aç®¡ç†æ¨¡å‹...")
        logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        logger.info(f"è®­ç»ƒè®¾å¤‡: {self.config.device}")
        
        best_val_loss = float('inf')
        patience = 0
        max_patience = 10
        
        for epoch in range(num_epochs):
            train_metrics = self.train_epoch(epoch)
            val_metrics = self.validate()
            
            logger.info(
                f"Epoch {epoch+1}/{num_epochs} - "
                f"Train Loss: {train_metrics['total_loss']:.4f}, "
                f"Val Loss: {val_metrics['val_loss']:.4f}, "
                f"LR: {self.optimizer.param_groups[0]['lr']:.2e}"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['val_loss'] < best_val_loss:
                best_val_loss = val_metrics['val_loss']
                self.model.save_model(f'a_manager_best_epoch_{epoch+1}.pth')
                patience = 0
            else:
                patience += 1
            
            # æ—©åœ
            if patience >= max_patience:
                logger.info(f"ğŸ›‘ æ—©åœäºepoch {epoch+1}")
                break
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.model.save_model('a_manager_final.pth')
        logger.info("âœ… Aç®¡ç†æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

class InteractiveAManager:
    """äº¤äº’å¼Aç®¡ç†æ¨¡å‹"""
    
    def __init__(self, model_path: str = None):
        self.config = ModelConfig()
        self.model = RealTrainableAManager(self.config)
        
        if model_path and os.path.exists(model_path):
            self.model.load_model(model_path)
            logger.info(f"å·²åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: {model_path}")
        
        self.model.eval()
        self.emotion_history = []
        self.task_history = []
    
    def process_task(self, task_type: str, description: str, 
                    context: Optional[Dict] = None) -> Dict[str, any]:
        """å¤„ç†ä»»åŠ¡è¯·æ±‚"""
        
        # ç¼–ç è¾“å…¥
        input_tensor = self.encode_input(description, task_type)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.model(input_tensor, context)
        
        # è§£ç ç»“æœ
        results = self.decode_outputs(outputs)
        
        # æ›´æ–°å†å²
        self.emotion_history.append(outputs['emotions'].cpu().numpy())
        self.task_history.append({
            'type': task_type,
            'description': description,
            'timestamp': datetime.now().isoformat()
        })
        
        return results
    
    def encode_input(self, text: str, task_type: str) -> torch.Tensor:
        """ç¼–ç è¾“å…¥ä¸ºå¼ é‡"""
        # ç®€åŒ–çš„æ–‡æœ¬ç¼–ç 
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„BERTæˆ–å…¶ä»–æ¨¡å‹
        encoded = torch.randn(1, 50, 768)  # æ¨¡æ‹Ÿç¼–ç 
        return encoded.to(self.config.device)
    
    def decode_outputs(self, outputs: Dict[str, torch.Tensor]) -> Dict[str, any]:
        """è§£ç æ¨¡å‹è¾“å‡º"""
        emotion_labels = ['å–œæ‚¦', 'æ‚²ä¼¤', 'æ„¤æ€’', 'ææƒ§', 'æƒŠè®¶', 'åŒæ¶', 'ä¿¡ä»»', 'æœŸå¾…']
        
        emotions = outputs['emotions'].cpu().numpy()[0]
        model_weights = outputs['model_weights'].cpu().numpy()[0]
        confidence = outputs['confidence'].cpu().numpy()[0][0]
        
        return {
            'emotions': {emotion_labels[i]: float(emotions[i]) for i in range(8)},
            'model_selection': {
                f'Model_{i}': float(weight) for i, weight in enumerate(model_weights)
            },
            'confidence': float(confidence),
            'timestamp': datetime.now().isoformat()
        }
    
    def get_emotional_state(self) -> Dict[str, float]:
        """è·å–å½“å‰æƒ…æ„ŸçŠ¶æ€"""
        if not self.emotion_history:
            return {
                'å–œæ‚¦': 0.3, 'æ‚²ä¼¤': 0.1, 'æ„¤æ€’': 0.05, 'ææƒ§': 0.05,
                'æƒŠè®¶': 0.2, 'åŒæ¶': 0.05, 'ä¿¡ä»»': 0.15, 'æœŸå¾…': 0.1
            }
        
        # è®¡ç®—å¹³å‡æƒ…æ„ŸçŠ¶æ€
        avg_emotions = np.mean(self.emotion_history[-10:], axis=0)
        emotion_labels = ['å–œæ‚¦', 'æ‚²ä¼¤', 'æ„¤æ€’', 'ææƒ§', 'æƒŠè®¶', 'åŒæ¶', 'ä¿¡ä»»', 'æœŸå¾…']
        
        return {emotion_labels[i]: float(avg_emotions[i]) for i in range(8)}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ çœŸå®å¯è®­ç»ƒçš„Aç®¡ç†æ¨¡å‹")
    print("=" * 50)
    
    # åˆå§‹åŒ–é…ç½®
    config = ModelConfig()
    print(f"è®¾å¤‡: {config.device}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = RealTrainableAManager(config)
    print(f"å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒå™¨
    trainer = AManagerTrainer(model, config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # æµ‹è¯•äº¤äº’
    interactive = InteractiveAManager('a_manager_final.pth')
    
    # ç¤ºä¾‹ä»»åŠ¡
    result = interactive.process_task('text', 'è¯·åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿ')
    print("\næµ‹è¯•ç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()