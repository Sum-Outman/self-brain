# -*- coding: utf-8 -*-
# Copyright 2025 The AI Management System Authors
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
ç»Ÿä¸€å¤§è¯­è¨€æ¨¡å‹å®ç° | Unified Large Language Model Implementation
æ”¯æŒæ ‡å‡†æ¨¡å¼å’Œå¢å¼ºæ¨¡å¼ï¼Œå…·æœ‰å¤šè¯­è¨€äº¤äº’ã€æƒ…æ„Ÿæ¨ç†ã€è‡ªä¸»å­¦ä¹ èƒ½åŠ›
(Supports standard and enhanced modes, with multilingual interaction, emotional reasoning, and self-learning capabilities)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import (
    AutoModel, 
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    GenerationConfig
)
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
import json
import logging
from datetime import datetime
import requests
import psutil
import gc
from pathlib import Path

# è®¾ç½®æ—¥å¿— | Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("unified_language_model.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("UnifiedLanguageModel")

class UnifiedMultilingualEmotionalLLM(nn.Module):
    """
    ç»Ÿä¸€å¤šè¯­è¨€æƒ…æ„Ÿå¤§è¯­è¨€æ¨¡å‹
    (Unified Multilingual Emotional Large Language Model)
    æ”¯æŒæ ‡å‡†æ¨¡å¼å’Œå¢å¼ºæ¨¡å¼ï¼Œé€šè¿‡é…ç½®æ§åˆ¶åŠŸèƒ½çº§åˆ«
    (Supports standard and enhanced modes, controlled by configuration)
    """
    
    def __init__(self, model_config: Dict = None, mode: str = "standard"):
        """
        åˆå§‹åŒ–ç»Ÿä¸€å¤šè¯­è¨€æƒ…æ„Ÿæ¨¡å‹
        (Initialize unified multilingual emotional model)
        
        å‚æ•° Parameters:
        model_config: æ¨¡å‹é…ç½®å­—å…¸ | Model configuration dictionary
        mode: è¿è¡Œæ¨¡å¼ - "standard" æˆ– "enhanced" | Operation mode - "standard" or "enhanced"
        """
        super().__init__()
        
        # è®¾ç½®è¿è¡Œæ¨¡å¼ | Set operation mode
        self.mode = mode
        
        # åŠ è½½é…ç½® | Load configuration
        self.config = self._get_default_config()
        if model_config:
            self.config.update(model_config)
        
        # åˆå§‹åŒ–å˜é‡ | Initialize variables
        self.external_models = {}
        self.self_learning_data = []
        self.training_history = []
        self.performance_metrics = {}
        self.emotion_intensity_cache = {}
        
        # æƒ…æ„Ÿåˆ†ç±» | Emotion categories (æ ‡å‡†ç‰ˆå’Œå¢å¼ºç‰ˆ)
        self.emotion_categories = self._get_emotion_categories()
        
        # æ”¯æŒçš„è¯­è¨€ | Supported languages
        self.supported_languages = self._get_supported_languages()
        
        # åŠ è½½åŸºç¡€æ¨¡å‹ | Load base model
        self._load_base_model()
        
        # å¢å¼ºæ¨¡å¼ç‰¹å®šåˆå§‹åŒ– | Enhanced mode specific initialization
        if self.mode == "enhanced":
            self._init_enhanced_features()
        
        logger.info(f"ç»Ÿä¸€å¤šè¯­è¨€æƒ…æ„Ÿå¤§è¯­è¨€æ¨¡å‹åˆå§‹åŒ–å®Œæˆ | Unified multilingual emotional LLM initialized (Mode: {self.mode})")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½® | Get default configuration"""
        return {
            "base_model": "xlm-roberta-base",
            "max_seq_length": 512,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.1,
            "num_beams": 4,
            "early_stopping": True,
            "emotion_analysis": True,
            "multilingual_support": True,
            "mode": self.mode,
            "self_learning": {
                "enabled": self.mode == "enhanced",
                "confidence_threshold": 0.7,
                "learning_rate": 1e-6,
                "optimization_frequency": 100,
                "memory_retention_days": 30
            },
            "external_apis": {
                "openai": {"enabled": False},
                "huggingface": {"enabled": False},
                "local": {"enabled": True}
            }
        }
    
    def _get_emotion_categories(self) -> Dict:
        """è·å–æƒ…æ„Ÿåˆ†ç±» | Get emotion categories"""
        if self.mode == "enhanced":
            # å¢å¼ºç‰ˆæƒ…æ„Ÿåˆ†ç±» | Enhanced emotion categories
            return {
                "anger": 0, "disgust": 1, "fear": 2, "joy": 3, "neutral": 4,
                "sadness": 5, "surprise": 6, "excitement": 7, "confusion": 8,
                "curiosity": 9, "love": 10, "gratitude": 11, "pride": 12,
                "shame": 13, "anxiety": 14, "contempt": 15, "amusement": 16,
                "awe": 17, "contentment": 18, "embarrassment": 19, "envy": 20
            }
        else:
            # æ ‡å‡†ç‰ˆæƒ…æ„Ÿåˆ†ç±» | Standard emotion categories
            return {
                "anger": 0, "disgust": 1, "fear": 2, "joy": 3, "neutral": 4,
                "sadness": 5, "surprise": 6
            }
    
    def _get_supported_languages(self) -> List:
        """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ | Get supported languages list"""
        if self.mode == "enhanced":
            # å¢å¼ºç‰ˆæ”¯æŒè¯­è¨€ | Enhanced supported languages
            return [
                "zh", "en", "de", "ja", "ru", "fr", "es", "it", "ko", "ar", 
                "pt", "hi", "bn", "vi", "th", "tr", "nl", "sv", "da", "no"
            ]
        else:
            # æ ‡å‡†ç‰ˆæ”¯æŒè¯­è¨€ | Standard supported languages
            return ["zh", "en", "de", "ja", "ru"]
    
    def _load_base_model(self):
        """åŠ è½½åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹ | Load base pretrained model"""
        try:
            model_name = self.config["base_model"]
            
            # åŠ è½½tokenizer | Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹ | Load model
            self.base_model = AutoModel.from_pretrained(model_name)
            
            # æƒ…æ„Ÿåˆ†æå¤´ | Emotion analysis head
            self.emotion_head = nn.Linear(
                self.base_model.config.hidden_size, 
                len(self.emotion_categories)
            )
            
            # è¯­è¨€å»ºæ¨¡å¤´ | Language modeling head
            self.lm_head = nn.Linear(
                self.base_model.config.hidden_size, 
                self.tokenizer.vocab_size
            )
            
            logger.info(f"åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name} | Base model loaded successfully: {model_name}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e} | Model loading failed: {e}")
            raise
    
    def _init_enhanced_features(self):
        """åˆå§‹åŒ–å¢å¼ºç‰¹æ€§ | Initialize enhanced features"""
        if self.mode != "enhanced":
            return
        
        # æƒ…æ„Ÿå¼ºåº¦å›å½’å™¨ | Emotion intensity regressor
        self.intensity_regressor = nn.Linear(
            self.base_model.config.hidden_size,
            1
        )
        
        # å¤šè¯­è¨€é€‚é…å™¨ | Multilingual adapter
        self.language_adapter = nn.ModuleDict({
            lang: nn.Linear(self.base_model.config.hidden_size, 128)
            for lang in self.supported_languages[:5]  # ä¸»è¦è¯­è¨€é€‚é…å™¨
        })
        
        # æƒ…æ„Ÿå¼ºåº¦æ˜ å°„ | Emotion intensity mapping
        self.emotion_intensity_map = {
            "anger": {"min": 0.3, "max": 1.0, "default": 0.7},
            "disgust": {"min": 0.4, "max": 1.0, "default": 0.8},
            "fear": {"min": 0.2, "max": 1.0, "default": 0.6},
            "joy": {"min": 0.3, "max": 1.0, "default": 0.7},
            "neutral": {"min": 0.0, "max": 0.3, "default": 0.1},
            "sadness": {"min": 0.3, "max": 1.0, "default": 0.7},
            "surprise": {"min": 0.4, "max": 1.0, "default": 0.8},
        }
        
        # è¿æ¥åˆ°å¤–éƒ¨æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰| Connect to external models (if enabled)
        if self.config["external_apis"]["local"]["enabled"]:
            self._connect_to_external_models()
    
    def _connect_to_external_models(self):
        """è¿æ¥åˆ°å¤–éƒ¨æ¨¡å‹ | Connect to external models"""
        external_config = self.config["external_apis"]
        
        if external_config.get("openai", {}).get("enabled", False):
            self.external_models["openai"] = external_config["openai"]
            logger.info("OpenAI APIè¿æ¥å·²é…ç½® | OpenAI API connection configured")
        
        if external_config.get("huggingface", {}).get("enabled", False):
            self.external_models["huggingface"] = external_config["huggingface"]
            logger.info("HuggingFace APIè¿æ¥å·²é…ç½® | HuggingFace API connection configured")
        
        if external_config.get("local", {}).get("enabled", False):
            self.external_models["local"] = external_config["local"]
            logger.info("æœ¬åœ°APIè¿æ¥å·²é…ç½® | Local API connection configured")
    
    def forward(self, input_ids, attention_mask, language_code: str = "en"):
        """
        å‰å‘ä¼ æ’­
        (Forward propagation)
        
        å‚æ•° Parameters:
        input_ids: è¾“å…¥token ID | Input token IDs
        attention_mask: æ³¨æ„åŠ›æ©ç  | Attention mask
        language_code: è¯­è¨€ä»£ç  | Language code
        """
        # è·å–åŸºç¡€æ¨¡å‹è¾“å‡º | Get base model output
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=self.mode == "enhanced",
            output_attentions=self.mode == "enhanced"
        )
        
        sequence_output = outputs.last_hidden_state
        pooled_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else sequence_output[:, 0, :]
        
        # å¢å¼ºæ¨¡å¼ç‰¹æ€§ | Enhanced mode features
        if self.mode == "enhanced":
            # è¯­è¨€ç‰¹å®šé€‚é… | Language-specific adaptation
            if language_code in self.language_adapter:
                language_features = self.language_adapter[language_code](pooled_output)
                enhanced_features = torch.cat([pooled_output, language_features], dim=-1)
            else:
                enhanced_features = pooled_output
            
            # æƒ…æ„Ÿé¢„æµ‹ | Emotion prediction
            emotion_logits = self.emotion_head(enhanced_features)
            emotion_probs = F.softmax(emotion_logits, dim=-1)
            
            # æƒ…æ„Ÿå¼ºåº¦é¢„æµ‹ | Emotion intensity prediction
            intensity_scores = torch.sigmoid(self.intensity_regressor(enhanced_features))
        else:
            # æ ‡å‡†æ¨¡å¼ | Standard mode
            emotion_logits = self.emotion_head(pooled_output)
            emotion_probs = F.softmax(emotion_logits, dim=-1)
            intensity_scores = torch.tensor([0.5])  # é»˜è®¤å¼ºåº¦ | Default intensity
        
        # è¯­è¨€å»ºæ¨¡ | Language modeling
        lm_logits = self.lm_head(sequence_output)
        
        # è¿”å›ç»“æœ | Return results
        result = {
            "lm_logits": lm_logits,
            "emotion_logits": emotion_logits,
            "emotion_probs": emotion_probs,
            "intensity_scores": intensity_scores,
        }
        
        if self.mode == "enhanced":
            result.update({
                "hidden_states": outputs.hidden_states,
                "attentions": outputs.attentions
            })
        
        return result
    
    def predict(self, text: str, language: str = "en", context: Optional[Dict] = None) -> Dict:
        """
        ç”Ÿæˆé¢„æµ‹ï¼ˆå¸¦æƒ…æ„Ÿæ¨ç†ï¼‰
        (Generate predictions with emotional reasoning)
        
        å‚æ•° Parameters:
        text: è¾“å…¥æ–‡æœ¬ | Input text
        language: è¯­è¨€ä»£ç  | Language code
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯ | Context information
        
        è¿”å› Returns:
        åŒ…å«é¢„æµ‹ç»“æœã€æƒ…æ„Ÿåˆ†æã€ç½®ä¿¡åº¦ç­‰çš„å­—å…¸
        Dictionary containing prediction results, emotion analysis, confidence, etc.
        """
        try:
            # é¢„å¤„ç†è¾“å…¥ | Preprocess input
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=self.config["max_seq_length"]
            )
            
            # æ¨¡å‹æ¨ç† | Model inference
            with torch.no_grad():
                outputs = self.forward(
                    inputs["input_ids"],
                    inputs["attention_mask"],
                    language
                )
            
            # æƒ…æ„Ÿåˆ†æ | Emotion analysis
            emotion_id = torch.argmax(outputs["emotion_probs"]).item()
            emotion_name = list(self.emotion_categories.keys())[emotion_id]
            emotion_confidence = outputs["emotion_probs"][0, emotion_id].item()
            
            # æƒ…æ„Ÿå¼ºåº¦åˆ†æ | Emotion intensity analysis
            if self.mode == "enhanced":
                intensity_score = outputs["intensity_scores"][0].item()
                adjusted_intensity = self._adjust_emotion_intensity(emotion_name, intensity_score)
            else:
                adjusted_intensity = 0.5  # æ ‡å‡†æ¨¡å¼é»˜è®¤å¼ºåº¦ | Default intensity for standard mode
            
            # ç”Ÿæˆå“åº” | Generate response
            generated_response = self._generate_response(
                outputs["lm_logits"],
                emotion_name,
                adjusted_intensity,
                language
            )
            
            # æ„å»ºç»“æœ | Build results
            result = {
                "text": generated_response,
                "emotion": {
                    "name": emotion_name,
                    "confidence": emotion_confidence,
                    "intensity": adjusted_intensity,
                    "category": self._categorize_emotion(emotion_name) if self.mode == "enhanced" else "basic"
                },
                "language": language,
                "confidence": self._calculate_overall_confidence(outputs),
                "timestamp": datetime.now().isoformat(),
                "model_metadata": {
                    "model_type": self.config["base_model"],
                    "version": "2.0.0" if self.mode == "enhanced" else "1.0.0",
                    "mode": self.mode
                }
            }
            
            # è‡ªä¸»å­¦ä¹ æ•°æ®æ”¶é›† | Self-learning data collection
            if self.mode == "enhanced" and self.config["self_learning"]["enabled"] and emotion_confidence > 0.8:
                self._collect_self_learning_data(text, result, context)
            
            return result
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e} | Prediction failed: {e}")
            return self._fallback_prediction(text, language, e)
    
    def _generate_response(self, lm_logits, emotion_name, intensity, language):
        """ç”Ÿæˆæƒ…æ„Ÿå¢å¼ºçš„å“åº” | Generate emotion-enhanced response"""
        # è§£ç ç”Ÿæˆæ–‡æœ¬ | Decode generated text
        generated_ids = torch.argmax(lm_logits, dim=-1)
        base_response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        
        # æƒ…æ„Ÿå¢å¼º | Emotion enhancement
        if self.mode == "enhanced":
            enhanced_response = self._enhance_with_emotion(base_response, emotion_name, intensity, language)
        else:
            # æ ‡å‡†æ¨¡å¼æƒ…æ„Ÿå¢å¼º | Standard mode emotion enhancement
            if emotion_name == "joy":
                enhanced_response = f"ğŸ˜Š {base_response}"
            elif emotion_name == "sadness":
                enhanced_response = f"ğŸ˜¢ {base_response}"
            elif emotion_name == "anger":
                enhanced_response = f"ğŸ˜  {base_response}"
            else:
                enhanced_response = base_response
        
        return enhanced_response
    
    def _enhance_with_emotion(self, text, emotion, intensity, language):
        """ä½¿ç”¨æƒ…æ„Ÿå¢å¼ºæ–‡æœ¬ | Enhance text with emotion"""
        emotion_enhancements = {
            "joy": {
                "zh": ["ğŸ˜Š", "å¤ªæ£’äº†ï¼", "ä»¤äººå…´å¥‹ï¼"],
                "en": ["ğŸ˜Š", "Great!", "Exciting!"],
                "default": ["ğŸ˜Š", "Wonderful!"]
            },
            "sadness": {
                "zh": ["ğŸ˜¢", "å¾ˆé—æ†¾", "ä»¤äººéš¾è¿‡"],
                "en": ["ğŸ˜¢", "Sorry to hear that", "That's sad"],
                "default": ["ğŸ˜¢", "I understand"]
            },
            "anger": {
                "zh": ["ğŸ˜ ", "è¿™ç¡®å®ä»¤äººæ„¤æ€’", "ä¸å¯æ¥å—"],
                "en": ["ğŸ˜ ", "That's frustrating", "Unacceptable"],
                "default": ["ğŸ˜ ", "I see why you're upset"]
            },
        }
        
        enhancement = emotion_enhancements.get(emotion, {}).get(language, 
                      emotion_enhancements.get(emotion, {}).get("default", [""]))
        
        if intensity > 0.7:
            prefix = enhancement[0] + " " + enhancement[1] + " "
        elif intensity > 0.4:
            prefix = enhancement[0] + " "
        else:
            prefix = ""
        
        return prefix + text
    
    def _adjust_emotion_intensity(self, emotion_name, raw_intensity):
        """è°ƒæ•´æƒ…æ„Ÿå¼ºåº¦å¾—åˆ† | Adjust emotion intensity score"""
        emotion_config = self.emotion_intensity_map.get(emotion_name, {})
        min_intensity = emotion_config.get("min", 0.0)
        max_intensity = emotion_config.get("max", 1.0)
        
        # è°ƒæ•´åˆ°æƒ…æ„Ÿç‰¹å®šèŒƒå›´ | Adjust to emotion-specific range
        adjusted = min_intensity + (max_intensity - min_intensity) * raw_intensity
        return round(adjusted, 2)
    
    def _categorize_emotion(self, emotion_name):
        """æƒ…æ„Ÿåˆ†ç±» | Categorize emotion"""
        categories = {
            "positive": ["joy", "excitement", "love", "gratitude", "pride", "amusement", "awe", "contentment"],
            "negative": ["anger", "disgust", "fear", "sadness", "shame", "anxiety", "contempt", "envy", "embarrassment"],
            "neutral": ["neutral", "surprise", "confusion", "curiosity"]
        }
        
        for category, emotions in categories.items():
            if emotion_name in emotions:
                return category
        return "unknown"
    
    def _calculate_overall_confidence(self, outputs):
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦ | Calculate overall confidence"""
        emotion_conf = outputs["emotion_probs"].max().item()
        lm_conf = F.softmax(outputs["lm_logits"], dim=-1).max().item()
        return round((emotion_conf + lm_conf) / 2, 3)
    
    def _collect_self_learning_data(self, input_text, prediction_result, context):
        """æ”¶é›†è‡ªä¸»å­¦ä¹ æ•°æ® | Collect self-learning data"""
        if self.mode != "enhanced":
            return
        
        learning_data = {
            "input": input_text,
            "prediction": prediction_result,
            "context": context or {},
            "timestamp": datetime.now().isoformat(),
            "confidence": prediction_result["confidence"]
        }
        
        self.self_learning_data.append(learning_data)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªæˆ‘ä¼˜åŒ– | Check if self-optimization is needed
        if len(self.self_learning_data) >= self.config["self_learning"]["optimization_frequency"]:
            self._perform_self_optimization()
    
    def _perform_self_optimization(self):
        """æ‰§è¡Œè‡ªæˆ‘ä¼˜åŒ– | Perform self-optimization"""
        if self.mode != "enhanced" or not self.config["self_learning"]["enabled"]:
            return
        
        logger.info("å¼€å§‹è‡ªæˆ‘ä¼˜åŒ–... | Starting self-optimization...")
        
        try:
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„ä¼˜åŒ–é€»è¾‘
            # ä¾‹å¦‚ï¼šä½¿ç”¨æ”¶é›†çš„æ•°æ®è¿›è¡Œå¾®è°ƒ
            
            # è®°å½•ä¼˜åŒ–å†å²
            self.training_history.append({
                "timestamp": datetime.now().isoformat(),
                "data_points": len(self.self_learning_data),
                "optimization_type": "self_learning",
                "metrics": {"learning_rate": self.config["self_learning"]["learning_rate"]}
            })
            
            # æ¸…ç©ºéƒ¨åˆ†æ•°æ®ï¼ˆä¿ç•™ç”¨äºæŒç»­å­¦ä¹ ï¼‰
            retain_count = int(len(self.self_learning_data) * 0.2)
            self.self_learning_data = self.self_learning_data[-retain_count:]
            
            logger.info("è‡ªæˆ‘ä¼˜åŒ–å®Œæˆ | Self-optimization completed")
            
        except Exception as e:
            logger.error(f"è‡ªæˆ‘ä¼˜åŒ–å¤±è´¥: {e} | Self-optimization failed: {e}")
    
    def _fallback_prediction(self, text, language, error):
        """å¤‡ç”¨é¢„æµ‹æ–¹æ³• | Fallback prediction method"""
        return {
            "text": f"I encountered an error: {str(error)}. Please try again.",
            "emotion": {
                "name": "neutral",
                "confidence": 0.5,
                "intensity": 0.3,
                "category": "neutral"
            },
            "language": language,
            "confidence": 0.3,
            "timestamp": datetime.now().isoformat(),
            "error": str(error)
        }
    
    def get_status(self) -> Dict:
        """
        è·å–æ¨¡å‹çŠ¶æ€ä¿¡æ¯
        Get model status information
        """
        process = psutil.Process()
        memory_info = process.memory_info()
        
        gpu_memory = 0
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        status = {
            "status": "active",
            "model_type": self.config["base_model"],
            "mode": self.mode,
            "memory_usage_mb": memory_info.rss / 1024 / 1024,
            "gpu_memory_mb": gpu_memory,
            "parameters_count": sum(p.numel() for p in self.parameters()),
            "supported_languages": self.supported_languages,
            "emotion_categories": list(self.emotion_categories.keys()),
        }
        
        if self.mode == "enhanced":
            status.update({
                "self_learning": {
                    "enabled": self.config["self_learning"]["enabled"],
                    "data_points": len(self.self_learning_data),
                    "optimization_count": len(self.training_history)
                },
                "external_models_connected": list(self.external_models.keys()),
                "performance_metrics": self.performance_metrics
            })
        
        return status
    
    def connect_to_external_model(self, model_type: str, api_config: Dict) -> bool:
        """è¿æ¥åˆ°å¤–éƒ¨æ¨¡å‹ | Connect to external model"""
        if self.mode != "enhanced":
            logger.warning("å¤–éƒ¨æ¨¡å‹è¿æ¥ä»…åœ¨å¢å¼ºæ¨¡å¼ä¸‹å¯ç”¨ | External model connection only available in enhanced mode")
            return False
        
        try:
            self.external_models[model_type] = api_config
            logger.info(f"æˆåŠŸè¿æ¥åˆ°å¤–éƒ¨{model_type}æ¨¡å‹ | Successfully connected to external {model_type} model")
            return True
        except Exception as e:
            logger.error(f"è¿æ¥å¤–éƒ¨æ¨¡å‹å¤±è´¥: {e} | Failed to connect to external model: {e}")
            return False
    
    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹ | Save model"""
        save_data = {
            'model_state_dict': self.state_dict(),
            'config': self.config,
            'tokenizer': self.tokenizer,
            'emotion_categories': self.emotion_categories,
        }
        
        if self.mode == "enhanced":
            save_data.update({
                'self_learning_data': self.self_learning_data,
                'training_history': self.training_history
            })
        
        torch.save(save_data, path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {path} | Model saved: {path}")
    
    @classmethod
    def load_model(cls, path: str):
        """åŠ è½½æ¨¡å‹ | Load model"""
        try:
            checkpoint = torch.load(path)
            mode = checkpoint['config'].get('mode', 'standard')
            model = cls(checkpoint['config'], mode)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.tokenizer = checkpoint['tokenizer']
            model.emotion_categories = checkpoint['emotion_categories']
            
            if mode == "enhanced":
                model.self_learning_data = checkpoint.get('self_learning_data', [])
                model.training_history = checkpoint.get('training_history', [])
            
            logger.info(f"æ¨¡å‹å·²åŠ è½½: {path} | Model loaded: {path}")
            return model
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e} | Model loading failed: {e}")
            raise

# æ¨¡å‹å·¥å‚å‡½æ•° | Model factory functions
def create_unified_model(config_path: Optional[str] = None, mode: str = "standard") -> UnifiedMultilingualEmotionalLLM:
    """åˆ›å»ºç»Ÿä¸€æ¨¡å‹å®ä¾‹ | Create unified model instance"""
    config = {}
    if config_path and Path(config_path).exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    return UnifiedMultilingualEmotionalLLM(config, mode)

# å¿«é€Ÿæµ‹è¯•å‡½æ•° | Quick test function
def test_unified_model():
    """æµ‹è¯•ç»Ÿä¸€æ¨¡å‹ | Test unified model"""
    print("æµ‹è¯•ç»Ÿä¸€å¤§è¯­è¨€æ¨¡å‹... | Testing Unified Language Model...")
    
    # æµ‹è¯•æ ‡å‡†æ¨¡å¼ | Test standard mode
    print("\n=== æ ‡å‡†æ¨¡å¼æµ‹è¯• === | === Standard Mode Test ===")
    model_standard = create_unified_model(mode="standard")
    
    # æµ‹è¯•å¢å¼ºæ¨¡å¼ | Test enhanced mode
    print("\n=== å¢å¼ºæ¨¡å¼æµ‹è¯• === | === Enhanced Mode Test ===")
    model_enhanced = create_unified_model(mode="enhanced")
    
    # æµ‹è¯•é¢„æµ‹ | Test prediction
    test_texts = [
        ("è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼æˆ‘éå¸¸å–œæ¬¢ï¼", "zh"),
        ("I'm really happy with this service!", "en"),
        ("Das ist wirklich enttÃ¤uschend", "de"),
    ]
    
    for text, lang in test_texts:
        print(f"\nè¾“å…¥ | Input ({lang}): {text}")
        
        # æ ‡å‡†æ¨¡å¼é¢„æµ‹
        result_std = model_standard.predict(text, lang)
        print(f"æ ‡å‡†æ¨¡å¼è¾“å‡º | Standard Output: {result_std['text']}")
        print(f"æ ‡å‡†æ¨¡å¼æƒ…æ„Ÿ | Standard Emotion: {result_std['emotion']['name']}")
        
        # å¢å¼ºæ¨¡å¼é¢„æµ‹
        result_enh = model_enhanced.predict(text, lang)
        print(f"å¢å¼ºæ¨¡å¼è¾“å‡º | Enhanced Output: {result_enh['text']}")
        print(f"å¢å¼ºæ¨¡å¼æƒ…æ„Ÿ | Enhanced Emotion: {result_enh['emotion']['name']} (å¼ºåº¦ | Intensity: {result_enh['emotion']['intensity']})")
    
    # æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€ | Show model status
    print(f"\næ ‡å‡†æ¨¡å¼çŠ¶æ€ | Standard Model Status:")
    status_std = model_standard.get_status()
    print(f"æ”¯æŒè¯­è¨€ | Supported languages: {len(status_std['supported_languages'])}")
    print(f"æƒ…æ„Ÿåˆ†ç±» | Emotion categories: {len(status_std['emotion_categories'])}")
    
    print(f"\nå¢å¼ºæ¨¡å¼çŠ¶æ€ | Enhanced Model Status:")
    status_enh = model_enhanced.get_status()
    print(f"æ”¯æŒè¯­è¨€ | Supported languages: {len(status_enh['supported_languages'])}")
    print(f"æƒ…æ„Ÿåˆ†ç±» | Emotion categories: {len(status_enh['emotion_categories'])}")
    print(f"è‡ªä¸»å­¦ä¹ æ•°æ®ç‚¹ | Self-learning data points: {status_enh['self_learning']['data_points']}")

if __name__ == '__main__':
    test_unified_model()