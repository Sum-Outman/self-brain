# -*- coding: utf-8 -*-
# Copyright 2025 The AI Management System Authors
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
å¤§è¯­è¨€æ¨¡å‹å®ç° (Large Language Model Implementation)
å…·æœ‰å¤šè¯­è¨€äº¤äº’å’Œæƒ…æ„Ÿæ¨ç†èƒ½åŠ›
(Multilingual interaction and emotional reasoning capabilities)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import time
import random
from datetime import datetime
from collections import deque

class LocalTokenizer:
    """æœ¬åœ°å­—ç¬¦çº§åˆ†è¯å™¨"""
    def __init__(self):
        self.vocab = {}
        self.id_to_char = {}
        self.vocab_size = 0
        self.pad_token_id = 0
        self.build_vocab()
    
    def build_vocab(self):
        """æ„å»ºå­—ç¬¦è¯æ±‡è¡¨"""
        # åŸºæœ¬å­—ç¬¦é›†
        chars = ['<PAD>', '<UNK>']
        # æ·»åŠ ASCIIå¯æ‰“å°å­—ç¬¦
        for i in range(32, 127):
            chars.append(chr(i))
        # æ·»åŠ å¸¸è§ä¸­æ–‡å­—ç¬¦
        common_chinese = 'çš„ä¸€æ˜¯åœ¨ä¸äº†æœ‰å’Œäººè¿™ä¸­å¤§ä¸ºä¸Šä¸ªå›½æˆ‘ä»¥è¦ä»–æ—¶æ¥ç”¨ä»¬ç”Ÿåˆ°ä½œåœ°äºå‡ºå°±åˆ†å¯¹æˆä¼šå¯ä¸»å‘å¹´åŠ¨åŒå·¥ä¹Ÿèƒ½ä¸‹è¿‡å­è¯´äº§ç§é¢è€Œæ–¹åå¤šå®šè¡Œå­¦æ³•æ‰€æ°‘å¾—ç»åä¸‰ä¹‹è¿›ç€ç­‰éƒ¨åº¦å®¶ç”µåŠ›é‡Œå¦‚æ°´åŒ–é«˜è‡ªäºŒç†èµ·å°ç‰©ç°å®åŠ é‡éƒ½ä¸¤ä½“åˆ¶æœºå½“ä½¿ç‚¹ä»ä¸šæœ¬å»æŠŠæ€§å¥½åº”å¼€å®ƒåˆè¿˜å› ç”±å…¶äº›ç„¶å‰å¤–å¤©æ”¿å››æ—¥é‚£ç¤¾ä¹‰äº‹å¹³å½¢ç›¸å…¨è¡¨é—´æ ·ä¸å…³å„é‡æ–°çº¿å†…æ•°æ­£å¿ƒåä½ æ˜çœ‹åŸåˆä¹ˆåˆ©æ¯”æˆ–ä½†è´¨æ°”ç¬¬å‘é“å‘½æ­¤å˜æ¡åªæ²¡ç»“è§£é—®æ„å»ºæœˆå…¬æ— ç³»å†›å¾ˆæƒ…è€…æœ€ç«‹ä»£æƒ³å·²é€šå¹¶æç›´é¢˜å…šç¨‹å±•äº”æœæ–™è±¡å‘˜é©ä½å…¥å¸¸æ–‡æ€»æ¬¡å“å¼æ´»è®¾åŠç®¡ç‰¹ä»¶é•¿æ±‚è€å¤´åŸºèµ„è¾¹æµè·¯çº§å°‘å›¾å±±ç»Ÿæ¥çŸ¥è¾ƒå°†ç»„è§è®¡åˆ«å¥¹æ‰‹è§’æœŸæ ¹è®ºè¿å†œæŒ‡å‡ ä¹åŒºå¼ºæ”¾å†³è¥¿è¢«å¹²åšå¿…æˆ˜å…ˆå›åˆ™ä»»å–æ®å¤„é˜Ÿå—ç»™è‰²å…‰é—¨å³ä¿æ²»åŒ—é€ ç™¾è§„çƒ­é¢†ä¸ƒæµ·å£ä¸œå¯¼å™¨å‹å¿—ä¸–é‡‘å¢äº‰æµé˜¶æ²¹æ€æœ¯æäº¤å—è”ä»€è®¤å…­å…±æƒæ”¶è¯æ”¹æ¸…å·±ç¾å†é‡‡è½¬æ›´å•é£åˆ‡æ‰“ç™½æ•™é€ŸèŠ±å¸¦å®‰åœºèº«è½¦ä¾‹çœŸåŠ¡å…·ä¸‡æ¯ç›®è‡³è¾¾èµ°ç§¯ç¤ºè®®å£°æŠ¥æ–—å®Œç±»å…«ç¦»ååç¡®æ‰ç§‘å¼ ä¿¡é©¬èŠ‚è¯ç±³æ•´ç©ºå…ƒå†µä»Šé›†æ¸©ä¼ åœŸè®¸æ­¥ç¾¤å¹¿çŸ³è®°éœ€æ®µç ”ç•Œæ‹‰æ—å¾‹å«ä¸”ç©¶è§‚è¶Šç»‡è£…å½±ç®—ä½æŒéŸ³ä¼—ä¹¦å¸ƒå¤å®¹å„¿é¡»é™…å•†ééªŒè¿æ–­æ·±éš¾è¿‘çŸ¿åƒå‘¨å§”ç´ æŠ€å¤‡åŠåŠé’çœåˆ—ä¹ å“çº¦æ”¯èˆ¬å²æ„ŸåŠ³ä¾¿å›¢å¾€é…¸å†å¸‚å…‹ä½•é™¤æ¶ˆæ„åºœç§°å¤ªå‡†ç²¾å€¼å·ç‡æ—ç»´åˆ’é€‰æ ‡å†™å­˜å€™æ¯›äº²å¿«æ•ˆæ–¯é™¢æŸ¥æ±Ÿå‹çœ¼ç‹æŒ‰æ ¼å…»æ˜“ç½®æ´¾å±‚ç‰‡å§‹å´ä¸“çŠ¶è‚²å‚äº¬è¯†é€‚å±åœ†åŒ…ç«ä½è°ƒæ»¡å¿å±€ç…§å‚çº¢ç»†å¼•å¬è¯¥é“ä»·ä¸¥'
        chars.extend(list(common_chinese))
        
        # æ„å»ºè¯æ±‡è¡¨
        for idx, char in enumerate(chars):
            self.vocab[char] = idx
            self.id_to_char[idx] = char
        
        self.vocab_size = len(self.vocab)
    
    def encode(self, text, max_length=512):
        """ç¼–ç æ–‡æœ¬ä¸ºtoken ID"""
        tokens = []
        for char in text[:max_length]:
            tokens.append(self.vocab.get(char, self.vocab['<UNK>']))
        
        # å¡«å……åˆ°æœ€å¤§é•¿åº¦
        if len(tokens) < max_length:
            tokens.extend([self.pad_token_id] * (max_length - len(tokens)))
        
        return tokens
    
    def decode(self, token_ids, skip_special_tokens=False):
        """è§£ç token IDä¸ºæ–‡æœ¬"""
        text = ''
        for token_id in token_ids:
            if token_id in self.id_to_char:
                if skip_special_tokens and token_id in [self.pad_token_id, self.vocab.get('<UNK>', 1)]:
                    continue
                text += self.id_to_char[token_id]
        return text

    def __call__(self, text, return_tensors=None, truncation=True, max_length=512):
        """å®ç°ç±»ä¼¼HuggingFaceåˆ†è¯å™¨çš„è°ƒç”¨æ¥å£"""
        input_ids = self.encode(text, max_length)
        if return_tensors == "pt":
            return {"input_ids": torch.tensor([input_ids])}
        return {"input_ids": input_ids}
    
    def __len__(self):
        return self.vocab_size

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç å±‚"""
    def __init__(self, d_model, max_len=512):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class LocalMultilingualLLM(nn.Module):
    """
    æœ¬åœ°å¤šè¯­è¨€æƒ…æ„Ÿå¤§è¯­è¨€æ¨¡å‹
    (Local Multilingual Emotional Large Language Model)
    """
    def __init__(self, vocab_size=3000, d_model=256, nhead=8, num_layers=6, max_length=512):
        """
        åˆå§‹åŒ–æœ¬åœ°å¤šè¯­è¨€æƒ…æ„Ÿæ¨¡å‹
        (Initialize local multilingual emotional model)
        
        å‚æ•° Parameters:
        vocab_size: è¯æ±‡è¡¨å¤§å° (Vocabulary size)
        d_model: æ¨¡å‹ç»´åº¦ (Model dimension)
        nhead: æ³¨æ„åŠ›å¤´æ•° (Number of attention heads)
        num_layers: Transformerå±‚æ•° (Number of transformer layers)
        max_length: æœ€å¤§åºåˆ—é•¿åº¦ (Maximum sequence length)
        """
        super().__init__()
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_length)
        
        # Transformerç¼–ç å™¨å±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='gelu'
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # æƒ…æ„Ÿåˆ†æå±‚ (7ç§åŸºæœ¬æƒ…ç»ª)
        self.emotion_head = nn.Linear(d_model, 7)
        
        # è¯­è¨€è¾“å‡ºå±‚
        self.lm_head = nn.Linear(d_model, vocab_size)
        
        # æœ¬åœ°åˆ†è¯å™¨
        self.tokenizer = LocalTokenizer()
        
        # æ¨¡å‹é…ç½®
        self.config = {
            'vocab_size': vocab_size,
            'd_model': d_model,
            'nhead': nhead,
            'num_layers': num_layers,
            'max_length': max_length
        }
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
        
        # åˆå§‹åŒ–ç»Ÿè®¡è·Ÿè¸ªå±æ€§
        self.input_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'avg_response_time': 0,
            'last_hour_requests': 0,
            'language_distribution': {}
        }
        self.performance_metrics = {
            'inference_speed': 0,
            'accuracy': 0
        }
        self.last_activity = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        self._request_times = deque(maxlen=100)  # å­˜å‚¨æœ€è¿‘100ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´
        self.emotion_labels = ["anger", "disgust", "fear", "joy", "neutral", "sadness", "surprise"]
    
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, input_ids, attention_mask=None):
        """
        å‰å‘ä¼ æ’­
        (Forward propagation)
        
        å‚æ•° Parameters:
        input_ids: è¾“å…¥token ID (Input token IDs)
        attention_mask: æ³¨æ„åŠ›æ©ç  (Attention mask)
        """
        # è¯åµŒå…¥
        x = self.embedding(input_ids)
        
        # ä½ç½®ç¼–ç 
        x = self.pos_encoding(x.transpose(0, 1))
        
        # Transformerç¼–ç 
        if attention_mask is None:
            # åˆ›å»ºé»˜è®¤çš„æ³¨æ„åŠ›æ©ç 
            seq_len = input_ids.size(1)
            attention_mask = torch.ones(seq_len, seq_len, device=input_ids.device)
        
        sequence_output = self.transformer_encoder(x, attention_mask)
        sequence_output = sequence_output.transpose(0, 1)
        
        # æƒ…æ„Ÿé¢„æµ‹ (Emotion prediction)
        emotion_logits = self.emotion_head(sequence_output[:, 0, :])
        
        # è¯­è¨€å»ºæ¨¡ (Language modeling)
        lm_logits = self.lm_head(sequence_output)
        
        return lm_logits, emotion_logits

    def train_model(self, dataset, epochs=3, lr=1e-5):
        """
        è®­ç»ƒæ¨¡å‹
        (Train the model)
        
        å‚æ•° Parameters:
        dataset: è®­ç»ƒæ•°æ®é›† (Training dataset)
        epochs: è®­ç»ƒè½®æ•° (Number of training epochs)
        lr: å­¦ä¹ ç‡ (Learning rate)
        """
        optimizer = torch.optim.Adam(self.parameters(), lr=lr)
        criterion_lm = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)
        criterion_emotion = nn.CrossEntropyLoss()
        
        self.train()
        for epoch in range(epochs):
            total_loss = 0
            for batch in dataset:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                emotion_labels = batch['emotion_labels']
                lm_labels = batch['lm_labels']
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                lm_logits, emotion_logits = self.forward(input_ids, attention_mask)
                
                # è®¡ç®—æŸå¤±
                lm_loss = criterion_lm(lm_logits.view(-1, self.tokenizer.vocab_size), 
                                      lm_labels.view(-1))
                emotion_loss = criterion_emotion(emotion_logits, emotion_labels)
                loss = lm_loss + emotion_loss
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            print(f"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataset):.4f}")
        
        print("Training completed!")
                
    def predict(self, text, language='en'):
        """
        ç”Ÿæˆé¢„æµ‹ï¼ˆå¸¦æƒ…æ„Ÿæ¨ç†ï¼‰
        (Generate predictions with emotional reasoning)

        å‚æ•° Parameters:
        text: è¾“å…¥æ–‡æœ¬ (Input text)
        language: è¯­è¨€ä»£ç  (Language code)
        """
        # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # åŠ è½½å¯¹åº”è¯­è¨€èµ„æº
            self._load_language_resources(language)
            
            # è®°å½•æœ€åå¤„ç†çš„æç¤ºé•¿åº¦
            self.last_prompt_length = len(text.split())

            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            with torch.no_grad():
                lm_logits, emotion_logits = self.forward(**inputs)

            # æƒ…æ„Ÿæ¨ç† - å¢å¼ºç‰ˆ
            emotion_probs = torch.softmax(emotion_logits, dim=-1)
            emotions = ["anger", "disgust", "fear", "joy", "neutral", "sadness", "surprise"]
            
            # è·å–æƒ…æ„Ÿæ¦‚ç‡åˆ†å¸ƒå’Œä¸»è¦æƒ…æ„Ÿ
            emotion_distribution = {emotions[i]: emotion_probs[0][i].item() for i in range(len(emotions))}
            primary_emotion_id = torch.argmax(emotion_probs).item()
            primary_emotion = emotions[primary_emotion_id]
            
            # ç”Ÿæˆå“åº”ï¼ˆè€ƒè™‘æƒ…æ„Ÿå’Œè¯­è¨€ï¼‰
            generated_ids = torch.argmax(lm_logits, dim=-1)
            response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)

            # æƒ…æ„Ÿå¢å¼ºå“åº” - æ ¹æ®ä¸åŒè¯­è¨€å’Œæƒ…æ„Ÿç”Ÿæˆä¸åŒçš„å“åº”é£æ ¼
            if primary_emotion == "joy":
                response = self._enhance_with_emotion(response, "joy", language)
            elif primary_emotion == "sadness":
                response = self._enhance_with_emotion(response, "sadness", language)
            elif primary_emotion == "anger":
                response = self._enhance_with_emotion(response, "anger", language)
            elif primary_emotion == "fear":
                response = self._enhance_with_emotion(response, "fear", language)
            elif primary_emotion == "surprise":
                response = self._enhance_with_emotion(response, "surprise", language)
            elif primary_emotion == "disgust":
                response = self._enhance_with_emotion(response, "disgust", language)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            process_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_stats(success=True, process_time=process_time, language=language)
            
            return {
                "response": response,
                "primary_emotion": primary_emotion,
                "emotion_distribution": emotion_distribution,
                "confidence": float(emotion_probs[0][primary_emotion_id]),
                "language": language,
                "processing_time_ms": process_time
            }
            
        except Exception as e:
            # è®¡ç®—å¤„ç†æ—¶é—´
            process_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ - æ ‡è®°ä¸ºå¤±è´¥
            self._update_stats(success=False, process_time=process_time, language=language)
            
            import logging
            logging.error(f"Prediction error: {str(e)}")
            
            return {
                "response": "I'm sorry, I couldn't process your request at the moment.",
                "primary_emotion": "neutral",
                "emotion_distribution": {e: 0 for e in ["anger", "disgust", "fear", "joy", "neutral", "sadness", "surprise"]},
                "confidence": 0.0,
                "language": language,
                "processing_time_ms": process_time,
                "error": str(e)
            }
            
    def _enhance_with_emotion(self, text, emotion, language):
        """\æ ¹æ®æƒ…æ„Ÿå’Œè¯­è¨€å¢å¼ºæ–‡æœ¬å“åº”"""
        # ä¸ºä¸åŒè¯­è¨€å’Œæƒ…æ„Ÿå®šä¹‰å¢å¼ºæ¨¡å¼
        enhancements = {
            'joy': {
                'en': ["ğŸ˜Š ", "Great! ", "Wonderful! "],
                'zh': ["ğŸ˜Š ", "å¤ªæ£’äº†ï¼", "å¤ªå¥½äº†ï¼"],
                'ja': ["ğŸ˜Š ", "ã™ã°ã‚‰ã—ã„ï¼", "ã‚ˆã‹ã£ãŸï¼"],
                'de': ["ğŸ˜Š ", "Fantastisch! ", "Super! "],
                'fr': ["ğŸ˜Š ", "GÃ©nial! ", "Super! "]
            },
            'sadness': {
                'en': ["ğŸ˜¢ ", "I'm sorry to hear that. ", "That's unfortunate. "],
                'zh': ["ğŸ˜¢ ", "å¬åˆ°è¿™ä¸ªæˆ‘å¾ˆé—æ†¾ã€‚", "çœŸä¸å¹¸ã€‚"],
                'ja': ["ğŸ˜¢ ", "ãã‚Œã¯æ®‹å¿µã§ã™ã€‚", "å¤§å¤‰ã§ã™ã­ã€‚"],
                'de': ["ğŸ˜¢ ", "Das tut mir leid. ", "Schade. "],
                'fr': ["ğŸ˜¢ ", "Je suis dÃ©solÃ©. ", "C'est dommage. "]
            },
            'anger': {
                'en': ["ğŸ˜  ", "That's frustrating. ", "I understand your frustration. "],
                'zh': ["ğŸ˜  ", "è¿™ç¡®å®ä»¤äººæ²®ä¸§ã€‚", "æˆ‘ç†è§£ä½ çš„æ„Ÿå—ã€‚"],
                'ja': ["ğŸ˜  ", "ã‚¤ãƒ©ãƒƒã¨ã—ã¾ã™ã­ã€‚", "ãŠæ°—æŒã¡ã¯ã‚ã‹ã‚Šã¾ã™ã€‚"],
                'de': ["ğŸ˜  ", "Das ist frustrierend. ", "Ich verstehe Ihre Frustration. "],
                'fr': ["ğŸ˜  ", "C'est frustrant. ", "Je comprends votre frustration. "]
            },
            'fear': {
                'en': ["ğŸ˜¨ ", "I understand your concern. ", "Let's address this carefully. "],
                'zh': ["ğŸ˜¨ ", "æˆ‘ç†è§£ä½ çš„æ‹…å¿§ã€‚", "è®©æˆ‘ä»¬è°¨æ…å¤„ç†ã€‚"],
                'ja': ["ğŸ˜¨ ", "å¿ƒé…ã¯ã‚ã‹ã‚Šã¾ã™ã€‚", "æ…é‡ã«å¯¾å‡¦ã—ã¾ã—ã‚‡ã†ã€‚"],
                'de': ["ğŸ˜¨ ", "Ich verstehe Ihre Sorge. ", "Lassen Sie uns das sorgfÃ¤ltig angehen. "],
                'fr': ["ğŸ˜¨ ", "Je comprends votre inquiÃ©tude. ", "Traitons cela avec prÃ©caution. "]
            },
            'surprise': {
                'en': ["ğŸ˜² ", "Wow! ", "That's surprising! "],
                'zh': ["ğŸ˜² ", "å“‡ï¼", "çœŸä»¤äººæƒŠè®¶ï¼"],
                'ja': ["ğŸ˜² ", "ã‚ãï¼", "ã³ã£ãã‚Šã—ã¾ã—ãŸï¼"],
                'de': ["ğŸ˜² ", "Wow! ", "Das ist Ã¼berraschend! "],
                'fr': ["ğŸ˜² ", "Wow! ", "C'est surprenant! "]
            },
            'disgust': {
                'en': ["ğŸ˜’ ", "That's unpleasant. ", "That's not ideal. "],
                'zh': ["ğŸ˜’ ", "è¿™ä»¤äººä¸æ„‰å¿«ã€‚", "è¿™ä¸å¤ªç†æƒ³ã€‚"],
                'ja': ["ğŸ˜’ ", "æ°—æŒã¡æ‚ªã„ã§ã™ã­ã€‚", "ç†æƒ³çš„ã§ã¯ãªã„ã§ã™ã­ã€‚"],
                'de': ["ğŸ˜’ ", "Das ist unangenehm. ", "Das ist nicht ideal. "],
                'fr': ["ğŸ˜’ ", "C'est dÃ©sagrÃ©able. ", "Ce n'est pas idÃ©al. "]
            }
        }
        
        # è·å–é€‚åˆå½“å‰æƒ…æ„Ÿå’Œè¯­è¨€çš„å¢å¼ºå‰ç¼€
        if emotion in enhancements and language in enhancements[emotion]:
            prefixes = enhancements[emotion][language]
            prefix = random.choice(prefixes) if prefixes else ""
            return f"{prefix}{text}"
        
        return text

    def get_status(self):
        """
        è·å–æ¨¡å‹çŠ¶æ€ä¿¡æ¯
        Get model status information
        
        è¿”å› Returns:
        çŠ¶æ€å­—å…¸åŒ…å«æ¨¡å‹å¥åº·çŠ¶æ€ã€å†…å­˜ä½¿ç”¨ã€æ€§èƒ½æŒ‡æ ‡ç­‰
        Status dictionary containing model health, memory usage, performance metrics, etc.
        """
        import psutil
        import torch
        import time
        
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        process = psutil.Process()
        memory_info = process.memory_info()
        
        # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœå¯ç”¨ï¼‰
        gpu_memory = 0
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        param_count = sum(p.numel() for p in self.parameters())
        
        # è·å–å®é™…æ€§èƒ½æŒ‡æ ‡
        inference_speed = self.performance_metrics.get('inference_speed', 0)
        accuracy = self.performance_metrics.get('accuracy', 0)
        
        return {
            "status": "active",
            "memory_usage_mb": memory_info.rss / 1024 / 1024,
            "gpu_memory_mb": gpu_memory,
            "parameters_count": param_count,
            "last_activity": self.last_activity,
            "performance": {
                "inference_speed": f"{inference_speed:.2f} tokens/sec" if inference_speed > 0 else "measuring...",
                "accuracy": f"{accuracy:.2%}" if accuracy > 0 else "measuring..."
            }
        }

    def get_input_stats(self):
        """
        è·å–è¾“å…¥ç»Ÿè®¡ä¿¡æ¯
        Get input statistics
        
        è¿”å› Returns:
        è¾“å…¥ç»Ÿè®¡å­—å…¸åŒ…å«å¤„ç†é‡ã€æˆåŠŸç‡ç­‰
        Input statistics dictionary containing processing volume, success rate, etc.
        """
        # ä»å®é™…ä½¿ç”¨ä¸­æ”¶é›†ç»Ÿè®¡æ•°æ®
        total_requests = self.input_stats.get('total_requests', 0)
        successful_requests = self.input_stats.get('successful_requests', 0)
        failed_requests = total_requests - successful_requests
        avg_response_time = self.input_stats.get('avg_response_time', 0)
        last_hour_requests = self.input_stats.get('last_hour_requests', 0)
        language_distribution = self.input_stats.get('language_distribution', {})
        
        return {
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "failed_requests": failed_requests,
            "average_response_time_ms": avg_response_time,
            "last_hour_requests": last_hour_requests,
            "language_distribution": language_distribution
        }

    # åˆå§‹åŒ–æ¨¡å‹æ—¶æ·»åŠ ç»Ÿè®¡è·Ÿè¸ªå±æ€§
    def __post_init__(self):
        self.input_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'avg_response_time': 0,
            'last_hour_requests': 0,
            'language_distribution': {}
        }
        self.performance_metrics = {
            'inference_speed': 0,
            'accuracy': 0
        }
        self.last_activity = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        self._request_times = deque(maxlen=100)  # å­˜å‚¨æœ€è¿‘100ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´

    def _update_stats(self, success=True, process_time=None, language=None):
        """æ›´æ–°æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        # æ›´æ–°æ€»è¯·æ±‚æ•°
        self.input_stats['total_requests'] += 1
        
        # æ›´æ–°æˆåŠŸè¯·æ±‚æ•°
        if success:
            self.input_stats['successful_requests'] += 1
        
        # æ›´æ–°å“åº”æ—¶é—´ç»Ÿè®¡
        if process_time:
            self._request_times.append(process_time)
            self.input_stats['avg_response_time'] = sum(self._request_times) / len(self._request_times)
            # è®¡ç®—æ¨ç†é€Ÿåº¦
            if hasattr(self, 'last_prompt_length') and self.last_prompt_length > 0:
                self.performance_metrics['inference_speed'] = self.last_prompt_length / process_time
        
        # æ›´æ–°è¯­è¨€åˆ†å¸ƒ
        if language:
            if language not in self.input_stats['language_distribution']:
                self.input_stats['language_distribution'][language] = 0
            self.input_stats['language_distribution'][language] += 1
        
        # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
        self.last_activity = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

# æ¨¡å‹ä¿å­˜å’ŒåŠ è½½å‡½æ•° (Model save/load functions)
def save_model(model, path):
    """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶ (Save model to file)"""
    torch.save({
        'model_state_dict': model.state_dict(),
        'tokenizer': model.tokenizer,
        'emotion_labels': model.emotion_labels
    }, path)

def load_model(path, model_name="xlm-roberta-base"):
    """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹ (Load model from file)"""
    model = MultilingualEmotionalLLM(model_name)
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.tokenizer = checkpoint['tokenizer']
    model.emotion_labels = checkpoint['emotion_labels']
    return model

# æ–°å¢ï¼šè¯­è¨€èµ„æºç®¡ç†
def _load_language_resources(self, lang_code):
    """åŠ è½½æŒ‡å®šè¯­è¨€èµ„æº"""
    # è¿™é‡Œä¼šè¿æ¥åˆ°ä¸»ç³»ç»Ÿçš„è¯­è¨€èµ„æºç®¡ç†å™¨
    # å®é™…å®ç°éœ€è¦ä¸manager_model/language_resources.pyé›†æˆ
    print(f"Switched to {lang_code} language resources")
